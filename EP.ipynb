{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcca369f",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3112fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7679a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "001665b0",
   "metadata": {},
   "source": [
    "# CONFIGURA√á√ïES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b7c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER_TRAIN = \"treino\"\n",
    "\n",
    "FILES = [\n",
    "    \"train_literal_dinamico.csv\",\n",
    "    \"train_complexo_simples.csv\",\n",
    "    \"train_arcaico_moderno.csv\",\n",
    "]\n",
    "\n",
    "preprocess_params = {\n",
    "    \"lowercase\": True,\n",
    "    \"normalize_unicode\": False,\n",
    "    \"remove_extra_whitespace\": True,\n",
    "    \"remove_punct\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a49aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos Pipelines e Grades de Hiperpar√¢metros para Grid Search\n",
    "param_grids = {\n",
    "    'Naive Bayes': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('model', MultinomialNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__max_features': [3000, 5000, 10000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__min_df': [2, 5],\n",
    "            'vectorizer__max_df': [0.9, 0.95],\n",
    "            'model__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__max_features': [3000, 5000, 10000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__min_df': [2, 5],\n",
    "            'vectorizer__max_df': [0.9, 0.95],\n",
    "            'model__C': [0.1, 1.0, 10.0],\n",
    "            'model__solver': ['lbfgs', 'liblinear'],\n",
    "            'model__class_weight': ['balanced', None]\n",
    "        }\n",
    "    },\n",
    "    'SVM (LinearSVC)': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('model', LinearSVC(dual=False, random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__max_features': [3000, 5000, 10000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__min_df': [2, 5],\n",
    "            'vectorizer__max_df': [0.9, 0.95],\n",
    "            'model__C': [0.1, 1.0, 10.0],\n",
    "            'model__max_iter': [1000, 2000]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5515f",
   "metadata": {},
   "source": [
    "# AN√ÅLISE DE BALANCEAMENTO DOS DATASETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f36e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_balanceamento(file_name):\n",
    "    \"\"\"Fun√ß√£o para analisar o balanceamento de um dataset\"\"\"\n",
    "    path = os.path.join(BASE_FOLDER_TRAIN, file_name)\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"AN√ÅLISE ESTAT√çSTICA - {file_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Informa√ß√µes b√°sicas\n",
    "    print(f\"\\nüìä INFORMA√á√ïES GERAIS:\")\n",
    "    print(f\"   ‚Ä¢ Total de linhas: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Total de colunas: {len(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    # Verificar valores nulos\n",
    "    print(f\"\\nüîç VALORES NULOS:\")\n",
    "    print(f\"   ‚Ä¢ Coluna 'text': {df['text'].isna().sum()}\")\n",
    "    print(f\"   ‚Ä¢ Coluna 'style': {df['style'].isna().sum()}\")\n",
    "    \n",
    "    # Distribui√ß√£o das classes\n",
    "    print(f\"\\nüìà DISTRIBUI√á√ÉO DAS CLASSES:\")\n",
    "    contagem_classes = df['style'].value_counts()\n",
    "    print(contagem_classes)\n",
    "    \n",
    "    print(f\"\\nüìä PORCENTAGEM POR CLASSE:\")\n",
    "    porcentagem_classes = df['style'].value_counts(normalize=True) * 100\n",
    "    for classe, perc in porcentagem_classes.items():\n",
    "        count = contagem_classes[classe]\n",
    "        print(f\"   ‚Ä¢ {classe}: {count:,} ({perc:.2f}%)\")\n",
    "    \n",
    "    # Verificar balanceamento\n",
    "    print(f\"\\n‚öñÔ∏è BALANCEAMENTO:\")\n",
    "    razao = contagem_classes.max() / contagem_classes.min()\n",
    "    print(f\"   ‚Ä¢ Raz√£o maior/menor classe: {razao:.2f}x\")\n",
    "    if razao < 1.5:\n",
    "        print(f\"   ‚Ä¢ Status: ‚úÖ Dataset bem balanceado\")\n",
    "    elif razao < 3:\n",
    "        print(f\"   ‚Ä¢ Status: ‚ö†Ô∏è Dataset moderadamente desbalanceado\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Status: ‚ùå Dataset desbalanceado\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    return df, contagem_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f6d70c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AN√ÅLISE ESTAT√çSTICA - train_literal_dinamico.csv\n",
      "============================================================\n",
      "\n",
      "üìä INFORMA√á√ïES GERAIS:\n",
      "   ‚Ä¢ Total de linhas: 36,964\n",
      "   ‚Ä¢ Total de colunas: 2\n",
      "   ‚Ä¢ Colunas: ['text', 'style']\n",
      "\n",
      "üîç VALORES NULOS:\n",
      "   ‚Ä¢ Coluna 'text': 0\n",
      "   ‚Ä¢ Coluna 'style': 0\n",
      "\n",
      "üìà DISTRIBUI√á√ÉO DAS CLASSES:\n",
      "style\n",
      "literal     18482\n",
      "dinamico    18482\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä PORCENTAGEM POR CLASSE:\n",
      "   ‚Ä¢ literal: 18,482 (50.00%)\n",
      "   ‚Ä¢ dinamico: 18,482 (50.00%)\n",
      "\n",
      "‚öñÔ∏è BALANCEAMENTO:\n",
      "   ‚Ä¢ Raz√£o maior/menor classe: 1.00x\n",
      "   ‚Ä¢ Status: ‚úÖ Dataset bem balanceado\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AN√ÅLISE ESTAT√çSTICA - train_complexo_simples.csv\n",
      "============================================================\n",
      "\n",
      "üìä INFORMA√á√ïES GERAIS:\n",
      "   ‚Ä¢ Total de linhas: 33,422\n",
      "   ‚Ä¢ Total de colunas: 2\n",
      "   ‚Ä¢ Colunas: ['text', 'style']\n",
      "\n",
      "üîç VALORES NULOS:\n",
      "   ‚Ä¢ Coluna 'text': 1\n",
      "   ‚Ä¢ Coluna 'style': 0\n",
      "\n",
      "üìà DISTRIBUI√á√ÉO DAS CLASSES:\n",
      "style\n",
      "complexo    16711\n",
      "simples     16711\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä PORCENTAGEM POR CLASSE:\n",
      "   ‚Ä¢ complexo: 16,711 (50.00%)\n",
      "   ‚Ä¢ simples: 16,711 (50.00%)\n",
      "\n",
      "‚öñÔ∏è BALANCEAMENTO:\n",
      "   ‚Ä¢ Raz√£o maior/menor classe: 1.00x\n",
      "   ‚Ä¢ Status: ‚úÖ Dataset bem balanceado\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "AN√ÅLISE ESTAT√çSTICA - train_arcaico_moderno.csv\n",
      "============================================================\n",
      "\n",
      "üìä INFORMA√á√ïES GERAIS:\n",
      "   ‚Ä¢ Total de linhas: 36,884\n",
      "   ‚Ä¢ Total de colunas: 2\n",
      "   ‚Ä¢ Colunas: ['text', 'style']\n",
      "\n",
      "üîç VALORES NULOS:\n",
      "   ‚Ä¢ Coluna 'text': 0\n",
      "   ‚Ä¢ Coluna 'style': 0\n",
      "\n",
      "üìà DISTRIBUI√á√ÉO DAS CLASSES:\n",
      "style\n",
      "arcaico    18442\n",
      "moderno    18442\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä PORCENTAGEM POR CLASSE:\n",
      "   ‚Ä¢ arcaico: 18,442 (50.00%)\n",
      "   ‚Ä¢ moderno: 18,442 (50.00%)\n",
      "\n",
      "‚öñÔ∏è BALANCEAMENTO:\n",
      "   ‚Ä¢ Raz√£o maior/menor classe: 1.00x\n",
      "   ‚Ä¢ Status: ‚úÖ Dataset bem balanceado\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analisar todos os datasets\n",
    "resultados_analise = {}\n",
    "\n",
    "for file_name in FILES:\n",
    "    df, contagem = analisar_balanceamento(file_name)\n",
    "    resultados_analise[file_name] = {\n",
    "        'dataframe': df,\n",
    "        'contagem_classes': contagem\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22520141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUMO COMPARATIVO - TODOS OS DATASETS\n",
      "================================================================================\n",
      "\n",
      "         Dataset  Total Linhas Classe 1  Count 1 Classe 2  Count 2 Raz√£o       Status\n",
      "literal_dinamico         36964  literal    18482 dinamico    18482 1.00x ‚úÖ Balanceado\n",
      "complexo_simples         33422 complexo    16711  simples    16711 1.00x ‚úÖ Balanceado\n",
      " arcaico_moderno         36884  arcaico    18442  moderno    18442 1.00x ‚úÖ Balanceado\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Criar tabela resumo comparativa\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMO COMPARATIVO - TODOS OS DATASETS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "resumo_data = []\n",
    "for file_name, resultado in resultados_analise.items():\n",
    "    df = resultado['dataframe']\n",
    "    contagem = resultado['contagem_classes']\n",
    "    razao = contagem.max() / contagem.min()\n",
    "    \n",
    "    resumo_data.append({\n",
    "        'Dataset': file_name.replace('train_', '').replace('.csv', ''),\n",
    "        'Total Linhas': len(df),\n",
    "        'Classe 1': contagem.index[0],\n",
    "        'Count 1': contagem.values[0],\n",
    "        'Classe 2': contagem.index[1],\n",
    "        'Count 2': contagem.values[1],\n",
    "        'Raz√£o': f\"{razao:.2f}x\",\n",
    "        'Status': '‚úÖ Balanceado' if razao < 1.5 else '‚ö†Ô∏è Moderado' if razao < 3 else '‚ùå Desbalanceado'\n",
    "    })\n",
    "\n",
    "df_resumo = pd.DataFrame(resumo_data)\n",
    "print(df_resumo.to_string(index=False))\n",
    "print()\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c9bcd",
   "metadata": {},
   "source": [
    "# PR√â-PROCESSAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b8d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_operations(text, params):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    if params.get(\"normalize_unicode\", True):\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "    if params.get(\"lowercase\", True):\n",
    "        text = text.lower()\n",
    "    if params.get(\"remove_punct\", False):\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    if params.get(\"remove_extra_whitespace\", True):\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Aviso: {path} n√£o encontrado.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    col_text, col_label = \"text\", \"style\"\n",
    "\n",
    "    df = df[[col_text, col_label]].dropna()\n",
    "    df = shuffle(df, random_state=10).reset_index(drop=True)\n",
    "\n",
    "    df[\"text_preproc\"] = df[col_text].apply(lambda x: preprocess_operations(x, preprocess_params))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[col_label])\n",
    "    X = df[\"text_preproc\"].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, stratify=y, random_state=10\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a3aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando: train_literal_dinamico.csv\n",
      "\n",
      "Processando: train_complexo_simples.csv\n",
      "\n",
      "Processando: train_arcaico_moderno.csv\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for file_name in FILES:\n",
    "    path = os.path.join(BASE_FOLDER_TRAIN, file_name)\n",
    "    print(f\"\\nProcessando: {file_name}\")\n",
    "    result = preprocess_data(path) \n",
    "\n",
    "    if result is not None:\n",
    "        X_train, X_test, y_train, y_test = result\n",
    "        datasets[file_name] = {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2991cd",
   "metadata": {},
   "source": [
    "# DATASET 1: ARCAICO vs MODERNO\n",
    "\n",
    "Classifica√ß√£o de textos entre estilo **arcaico** e **moderno**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9b11e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados - train_arcaico_moderno.csv\n",
      "   Treino: 31351 textos | Teste: 5533 textos\n"
     ]
    }
   ],
   "source": [
    "# Preparar dados - arcaico_moderno\n",
    "X_train = datasets[\"train_arcaico_moderno.csv\"][\"X_train\"]\n",
    "X_test = datasets[\"train_arcaico_moderno.csv\"][\"X_test\"]\n",
    "y_train = datasets[\"train_arcaico_moderno.csv\"][\"y_train\"]\n",
    "y_test = datasets[\"train_arcaico_moderno.csv\"][\"y_test\"]\n",
    "\n",
    "print(f\"Dados carregados - train_arcaico_moderno.csv\")\n",
    "print(f\"   Treino: {len(X_train)} textos | Teste: {len(X_test)} textos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c41ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH COM PIPELINE - ARCAICO vs MODERNO (10-FOLD CV)\n",
      "================================================================================\n",
      "Otimizando TF-IDF + Modelos simultaneamente...\n",
      "\n",
      "[Naive Bayes] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'alpha': 0.1}\n",
      "  ‚úì Acuracia (CV): 0.8362\n",
      "\n",
      "[Logistic Regression] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "  ‚úì Acuracia (CV): 0.8386\n",
      "\n",
      "[SVM (LinearSVC)] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'C': 0.1, 'max_iter': 1000}\n",
      "  ‚úì Acuracia (CV): 0.8357\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GRID SEARCH COM PIPELINE - ARCAICO vs MODERNO (10-FOLD CV)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Otimizando TF-IDF + Modelos simultaneamente...\\n\")\n",
    "\n",
    "# Armazenar melhores pipelines\n",
    "best_models = {}\n",
    "cv_results = {}\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f\"[{name}] Executando Grid Search...\")\n",
    "    print(f\"   Testando {len(config['params']['vectorizer__max_features']) * len(config['params']['vectorizer__ngram_range']) * len(config['params']['vectorizer__min_df']) * len(config['params']['vectorizer__max_df'])} combina√ß√µes de TF-IDF...\")\n",
    "    \n",
    "    # Grid Search com 10-fold CV\n",
    "    grid_search = GridSearchCV(\n",
    "        config['pipeline'],\n",
    "        config['params'],\n",
    "        cv=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Separar par√¢metros de TF-IDF e modelo\n",
    "    vectorizer_params = {k.replace('vectorizer__', ''): v \n",
    "                        for k, v in grid_search.best_params_.items() \n",
    "                        if k.startswith('vectorizer__')}\n",
    "    model_params = {k.replace('model__', ''): v \n",
    "                   for k, v in grid_search.best_params_.items() \n",
    "                   if k.startswith('model__')}\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'vectorizer_params': vectorizer_params,\n",
    "        'model_params': model_params,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'mean': grid_search.best_score_,\n",
    "        'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Melhores params TF-IDF: {vectorizer_params}\")\n",
    "    print(f\"  ‚úì Melhores params Modelo: {model_params}\")\n",
    "    print(f\"  ‚úì Acuracia (CV): {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd670390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste Final no Hold-Out - ARCAICO vs MODERNO\n",
      "============================================================\n",
      "Naive Bayes              : CV=0.8362 | Hold-Out=0.8332\n",
      "Logistic Regression      : CV=0.8386 | Hold-Out=0.8473\n",
      "SVM (LinearSVC)          : CV=0.8357 | Hold-Out=0.8417\n",
      "\n",
      "Melhor modelo (Hold-Out): Logistic Regression - 84.73%\n"
     ]
    }
   ],
   "source": [
    "# Teste Final - arcaico_moderno (com melhores params do Grid Search)\n",
    "print(\"\\nTeste Final no Hold-Out - ARCAICO vs MODERNO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_results = {}\n",
    "for name, pipeline in best_models.items():\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    final_results[name] = {'holdout_acc': acc, 'cv_mean': cv_results[name]['mean']}\n",
    "    print(f\"{name:25s}: CV={cv_results[name]['mean']:.4f} | Hold-Out={acc:.4f}\")\n",
    "\n",
    "best_final = max(final_results.items(), key=lambda x: x[1]['holdout_acc'])\n",
    "print(f\"\\nMelhor modelo (Hold-Out): {best_final[0]} - {best_final[1]['holdout_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f110d7b1",
   "metadata": {},
   "source": [
    "# DATASET 2: COMPLEXO vs SIMPLES\n",
    "\n",
    "Classifica√ß√£o de textos entre estilo **complexo** e **simples**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71362410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados - train_complexo_simples.csv\n",
      "   Treino: 28407 textos | Teste: 5014 textos\n"
     ]
    }
   ],
   "source": [
    "# Preparar dados - complexo_simples\n",
    "X_train_cs = datasets[\"train_complexo_simples.csv\"][\"X_train\"]\n",
    "X_test_cs = datasets[\"train_complexo_simples.csv\"][\"X_test\"]\n",
    "y_train_cs = datasets[\"train_complexo_simples.csv\"][\"y_train\"]\n",
    "y_test_cs = datasets[\"train_complexo_simples.csv\"][\"y_test\"]\n",
    "\n",
    "print(f\"Dados carregados - train_complexo_simples.csv\")\n",
    "print(f\"   Treino: {len(X_train_cs)} textos | Teste: {len(X_test_cs)} textos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02fc66bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH COM PIPELINE - COMPLEXO vs SIMPLES (10-FOLD CV)\n",
      "================================================================================\n",
      "Otimizando TF-IDF + Modelos simultaneamente...\n",
      "\n",
      "[Naive Bayes] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'alpha': 0.1}\n",
      "  ‚úì Acuracia (CV): 0.8149\n",
      "\n",
      "[Logistic Regression] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "  ‚úì Acuracia (CV): 0.8346\n",
      "\n",
      "[SVM (LinearSVC)] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'C': 0.1, 'max_iter': 1000}\n",
      "  ‚úì Acuracia (CV): 0.8301\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GRID SEARCH COM PIPELINE - COMPLEXO vs SIMPLES (10-FOLD CV)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Otimizando TF-IDF + Modelos simultaneamente...\\n\")\n",
    "\n",
    "# Armazenar melhores pipelines\n",
    "best_models_cs = {}\n",
    "cv_results_cs = {}\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f\"[{name}] Executando Grid Search...\")\n",
    "    print(f\"   Testando {len(config['params']['vectorizer__max_features']) * len(config['params']['vectorizer__ngram_range']) * len(config['params']['vectorizer__min_df']) * len(config['params']['vectorizer__max_df'])} combina√ß√µes de TF-IDF...\")\n",
    "    \n",
    "    # Grid Search com 10-fold CV\n",
    "    grid_search = GridSearchCV(\n",
    "        config['pipeline'],\n",
    "        config['params'],\n",
    "        cv=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_cs, y_train_cs)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    best_models_cs[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Separar par√¢metros de TF-IDF e modelo\n",
    "    vectorizer_params = {k.replace('vectorizer__', ''): v \n",
    "                        for k, v in grid_search.best_params_.items() \n",
    "                        if k.startswith('vectorizer__')}\n",
    "    model_params = {k.replace('model__', ''): v \n",
    "                   for k, v in grid_search.best_params_.items() \n",
    "                   if k.startswith('model__')}\n",
    "    \n",
    "    cv_results_cs[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'vectorizer_params': vectorizer_params,\n",
    "        'model_params': model_params,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'mean': grid_search.best_score_,\n",
    "        'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Melhores params TF-IDF: {vectorizer_params}\")\n",
    "    print(f\"  ‚úì Melhores params Modelo: {model_params}\")\n",
    "    print(f\"  ‚úì Acuracia (CV): {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6471c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste Final no Hold-Out - COMPLEXO vs SIMPLES\n",
      "============================================================\n",
      "Naive Bayes              : CV=0.8149 | Hold-Out=0.8145\n",
      "Logistic Regression      : CV=0.8346 | Hold-Out=0.8311\n",
      "SVM (LinearSVC)          : CV=0.8301 | Hold-Out=0.8221\n",
      "\n",
      "Melhor modelo (Hold-Out): Logistic Regression - 83.11%\n"
     ]
    }
   ],
   "source": [
    "# Teste Final - complexo_simples (com melhores params do Grid Search)\n",
    "print(\"\\nTeste Final no Hold-Out - COMPLEXO vs SIMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_results_cs = {}\n",
    "for name, pipeline in best_models_cs.items():\n",
    "    y_pred = pipeline.predict(X_test_cs)\n",
    "    acc = accuracy_score(y_test_cs, y_pred)\n",
    "    final_results_cs[name] = {'holdout_acc': acc, 'cv_mean': cv_results_cs[name]['mean']}\n",
    "    print(f\"{name:25s}: CV={cv_results_cs[name]['mean']:.4f} | Hold-Out={acc:.4f}\")\n",
    "\n",
    "best_final_cs = max(final_results_cs.items(), key=lambda x: x[1]['holdout_acc'])\n",
    "print(f\"\\nMelhor modelo (Hold-Out): {best_final_cs[0]} - {best_final_cs[1]['holdout_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068aadf",
   "metadata": {},
   "source": [
    "# DATASET 3: LITERAL vs DIN√ÇMICO\n",
    "\n",
    "Classifica√ß√£o de textos entre estilo **literal** e **din√¢mico**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab861158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados - train_literal_dinamico.csv\n",
      "   Treino: 31419 textos | Teste: 5545 textos\n"
     ]
    }
   ],
   "source": [
    "# Preparar dados - literal_dinamico\n",
    "X_train_ld = datasets[\"train_literal_dinamico.csv\"][\"X_train\"]\n",
    "X_test_ld = datasets[\"train_literal_dinamico.csv\"][\"X_test\"]\n",
    "y_train_ld = datasets[\"train_literal_dinamico.csv\"][\"y_train\"]\n",
    "y_test_ld = datasets[\"train_literal_dinamico.csv\"][\"y_test\"]\n",
    "\n",
    "print(f\"Dados carregados - train_literal_dinamico.csv\")\n",
    "print(f\"   Treino: {len(X_train_ld)} textos | Teste: {len(X_test_ld)} textos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f1b6770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRID SEARCH COM PIPELINE - LITERAL vs DIN√ÇMICO (10-FOLD CV)\n",
      "================================================================================\n",
      "Otimizando TF-IDF + Modelos simultaneamente...\n",
      "\n",
      "[Naive Bayes] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'alpha': 0.1}\n",
      "  ‚úì Acuracia (CV): 0.8355\n",
      "\n",
      "[Logistic Regression] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'C': 1.0, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "  ‚úì Acuracia (CV): 0.8347\n",
      "\n",
      "[SVM (LinearSVC)] Executando Grid Search...\n",
      "   Testando 24 combina√ß√µes de TF-IDF...\n",
      "  ‚úì Melhores params TF-IDF: {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "  ‚úì Melhores params Modelo: {'C': 0.1, 'max_iter': 1000}\n",
      "  ‚úì Acuracia (CV): 0.8341\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GRID SEARCH COM PIPELINE - LITERAL vs DIN√ÇMICO (10-FOLD CV)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Otimizando TF-IDF + Modelos simultaneamente...\\n\")\n",
    "\n",
    "# Armazenar melhores pipelines\n",
    "best_models_ld = {}\n",
    "cv_results_ld = {}\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f\"[{name}] Executando Grid Search...\")\n",
    "    print(f\"   Testando {len(config['params']['vectorizer__max_features']) * len(config['params']['vectorizer__ngram_range']) * len(config['params']['vectorizer__min_df']) * len(config['params']['vectorizer__max_df'])} combina√ß√µes de TF-IDF...\")\n",
    "    \n",
    "    # Grid Search com 10-fold CV\n",
    "    grid_search = GridSearchCV(\n",
    "        config['pipeline'],\n",
    "        config['params'],\n",
    "        cv=10,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_ld, y_train_ld)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    best_models_ld[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Separar par√¢metros de TF-IDF e modelo\n",
    "    vectorizer_params = {k.replace('vectorizer__', ''): v \n",
    "                        for k, v in grid_search.best_params_.items() \n",
    "                        if k.startswith('vectorizer__')}\n",
    "    model_params = {k.replace('model__', ''): v \n",
    "                   for k, v in grid_search.best_params_.items() \n",
    "                   if k.startswith('model__')}\n",
    "    \n",
    "    cv_results_ld[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'vectorizer_params': vectorizer_params,\n",
    "        'model_params': model_params,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'mean': grid_search.best_score_,\n",
    "        'std': grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Melhores params TF-IDF: {vectorizer_params}\")\n",
    "    print(f\"  ‚úì Melhores params Modelo: {model_params}\")\n",
    "    print(f\"  ‚úì Acuracia (CV): {grid_search.best_score_:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d1629c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste Final no Hold-Out - LITERAL vs DIN√ÇMICO\n",
      "============================================================\n",
      "Naive Bayes              : CV=0.8355 | Hold-Out=0.8267\n",
      "Logistic Regression      : CV=0.8347 | Hold-Out=0.8357\n",
      "SVM (LinearSVC)          : CV=0.8341 | Hold-Out=0.8368\n",
      "\n",
      "Melhor modelo (Hold-Out): SVM (LinearSVC) - 83.68%\n"
     ]
    }
   ],
   "source": [
    "# Teste Final - literal_dinamico (com melhores params do Grid Search)\n",
    "print(\"\\nTeste Final no Hold-Out - LITERAL vs DIN√ÇMICO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_results_ld = {}\n",
    "for name, pipeline in best_models_ld.items():\n",
    "    y_pred = pipeline.predict(X_test_ld)\n",
    "    acc = accuracy_score(y_test_ld, y_pred)\n",
    "    final_results_ld[name] = {'holdout_acc': acc, 'cv_mean': cv_results_ld[name]['mean']}\n",
    "    print(f\"{name:25s}: CV={cv_results_ld[name]['mean']:.4f} | Hold-Out={acc:.4f}\")\n",
    "\n",
    "best_final_ld = max(final_results_ld.items(), key=lambda x: x[1]['holdout_acc'])\n",
    "print(f\"\\nMelhor modelo (Hold-Out): {best_final_ld[0]} - {best_final_ld[1]['holdout_acc']*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e685c0",
   "metadata": {},
   "source": [
    "# COMPARA√á√ÉO FINAL - TODOS OS DATASETS\n",
    "\n",
    "An√°lise comparativa do desempenho dos modelos nos 3 tipos de classifica√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22c23abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "COMPARA√á√ÉO FINAL - TODOS OS DATASETS\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "DATASET: ARCAICO vs MODERNO\n",
      "====================================================================================================\n",
      "\n",
      "ü•á Logistic Regression\n",
      "   Acur√°cia CV:       0.8386 (83.86%)\n",
      "   Acur√°cia Hold-Out: 0.8473 (84.73%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "\n",
      "ü•à SVM (LinearSVC)\n",
      "   Acur√°cia CV:       0.8357 (83.57%)\n",
      "   Acur√°cia Hold-Out: 0.8417 (84.17%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 0.1, 'max_iter': 1000}\n",
      "\n",
      "ü•â Naive Bayes\n",
      "   Acur√°cia CV:       0.8362 (83.62%)\n",
      "   Acur√°cia Hold-Out: 0.8332 (83.32%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'alpha': 0.1}\n",
      "\n",
      "====================================================================================================\n",
      "DATASET: COMPLEXO vs SIMPLES\n",
      "====================================================================================================\n",
      "\n",
      "ü•á Logistic Regression\n",
      "   Acur√°cia CV:       0.8346 (83.46%)\n",
      "   Acur√°cia Hold-Out: 0.8311 (83.11%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "\n",
      "ü•à SVM (LinearSVC)\n",
      "   Acur√°cia CV:       0.8301 (83.01%)\n",
      "   Acur√°cia Hold-Out: 0.8221 (82.21%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 0.1, 'max_iter': 1000}\n",
      "\n",
      "ü•â Naive Bayes\n",
      "   Acur√°cia CV:       0.8149 (81.49%)\n",
      "   Acur√°cia Hold-Out: 0.8145 (81.45%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'alpha': 0.1}\n",
      "\n",
      "====================================================================================================\n",
      "DATASET: LITERAL vs DIN√ÇMICO\n",
      "====================================================================================================\n",
      "\n",
      "ü•á SVM (LinearSVC)\n",
      "   Acur√°cia CV:       0.8341 (83.41%)\n",
      "   Acur√°cia Hold-Out: 0.8368 (83.68%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 0.1, 'max_iter': 1000}\n",
      "\n",
      "ü•à Logistic Regression\n",
      "   Acur√°cia CV:       0.8347 (83.47%)\n",
      "   Acur√°cia Hold-Out: 0.8357 (83.57%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 1.0, 'class_weight': 'balanced', 'solver': 'liblinear'}\n",
      "\n",
      "ü•â Naive Bayes\n",
      "   Acur√°cia CV:       0.8355 (83.55%)\n",
      "   Acur√°cia Hold-Out: 0.8267 (82.67%)\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'alpha': 0.1}\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "RESUMO - MELHOR MODELO POR DATASET\n",
      "====================================================================================================\n",
      "\n",
      "üèÜ ARCAICO vs MODERNO\n",
      "   Melhor Modelo:     Logistic Regression\n",
      "   Acur√°cia Hold-Out: 84.73%\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 5, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "\n",
      "üèÜ COMPLEXO vs SIMPLES\n",
      "   Melhor Modelo:     Logistic Regression\n",
      "   Acur√°cia Hold-Out: 83.11%\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n",
      "\n",
      "üèÜ LITERAL vs DIN√ÇMICO\n",
      "   Melhor Modelo:     SVM (LinearSVC)\n",
      "   Acur√°cia Hold-Out: 83.68%\n",
      "   Params TF-IDF:     {'max_df': 0.9, 'max_features': 10000, 'min_df': 2, 'ngram_range': (1, 2)}\n",
      "   Params Modelo:     {'C': 0.1, 'max_iter': 1000}\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"COMPARA√á√ÉO FINAL - TODOS OS DATASETS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Organizar resultados com par√¢metros\n",
    "datasets_comparison = {\n",
    "    'ARCAICO vs MODERNO': {\n",
    "        'results': final_results,\n",
    "        'cv_results': cv_results\n",
    "    },\n",
    "    'COMPLEXO vs SIMPLES': {\n",
    "        'results': final_results_cs,\n",
    "        'cv_results': cv_results_cs\n",
    "    },\n",
    "    'LITERAL vs DIN√ÇMICO': {\n",
    "        'results': final_results_ld,\n",
    "        'cv_results': cv_results_ld\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mostrar resultados por dataset\n",
    "for dataset_name, data in datasets_comparison.items():\n",
    "    results = data['results']\n",
    "    cv_res = data['cv_results']\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Ordenar por Hold-Out\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['holdout_acc'], reverse=True)\n",
    "    \n",
    "    for i, (model_name, metrics) in enumerate(sorted_results, 1):\n",
    "        emoji = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\"\n",
    "        print(f\"\\n{emoji} {model_name}\")\n",
    "        print(f\"   Acur√°cia CV:       {metrics['cv_mean']:.4f} ({metrics['cv_mean']*100:.2f}%)\")\n",
    "        print(f\"   Acur√°cia Hold-Out: {metrics['holdout_acc']:.4f} ({metrics['holdout_acc']*100:.2f}%)\")\n",
    "        print(f\"   Params TF-IDF:     {cv_res[model_name]['vectorizer_params']}\")\n",
    "        print(f\"   Params Modelo:     {cv_res[model_name]['model_params']}\")\n",
    "\n",
    "# Resumo final - Melhor modelo por dataset\n",
    "print(f\"\\n\\n{'='*100}\")\n",
    "print(\"RESUMO - MELHOR MODELO POR DATASET\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "for dataset_name, data in datasets_comparison.items():\n",
    "    results = data['results']\n",
    "    cv_res = data['cv_results']\n",
    "    \n",
    "    best = max(results.items(), key=lambda x: x[1]['holdout_acc'])\n",
    "    best_name = best[0]\n",
    "    best_metrics = best[1]\n",
    "    \n",
    "    print(f\"üèÜ {dataset_name}\")\n",
    "    print(f\"   Melhor Modelo:     {best_name}\")\n",
    "    print(f\"   Acur√°cia Hold-Out: {best_metrics['holdout_acc']*100:.2f}%\")\n",
    "    print(f\"   Params TF-IDF:     {cv_res[best_name]['vectorizer_params']}\")\n",
    "    print(f\"   Params Modelo:     {cv_res[best_name]['model_params']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c9fca",
   "metadata": {},
   "source": [
    "# PREDI√á√ïES NOS DADOS DE TESTE\n",
    "\n",
    "Aplica√ß√£o dos melhores modelos treinados nos dados de teste da pasta `teste/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f73cc6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CARREGANDO DADOS DE TESTE\n",
      "================================================================================\n",
      "Pasta: teste/\n",
      "Arquivos: 3\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o da pasta de teste\n",
    "BASE_FOLDER_TEST = \"teste\"\n",
    "\n",
    "TEST_FILES = [\n",
    "    \"test_arcaico_moderno.csv\",\n",
    "    \"test_complexo_simples.csv\",\n",
    "    \"test_literal_dinamico.csv\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CARREGANDO DADOS DE TESTE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Pasta: {BASE_FOLDER_TEST}/\")\n",
    "print(f\"Arquivos: {len(TEST_FILES)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c95e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando: test_arcaico_moderno.csv\n",
      "  ‚úì Arquivo lido com encoding: latin-1\n",
      "  ‚úì 9,222 textos carregados e pr√©-processados\n",
      "\n",
      "Carregando: test_complexo_simples.csv\n",
      "  ‚úì Arquivo lido com encoding: latin-1\n",
      "  ‚úì 8,356 textos carregados e pr√©-processados\n",
      "\n",
      "Carregando: test_literal_dinamico.csv\n",
      "  ‚úì Arquivo lido com encoding: latin-1\n",
      "  ‚úì 9,242 textos carregados e pr√©-processados\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_test(path):\n",
    "    \"\"\"Carrega e pr√©-processa dados de teste (sem labels)\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Aviso: {path} n√£o encontrado.\")\n",
    "        return None\n",
    "    \n",
    "    # Tentar diferentes encodings comuns no Windows\n",
    "    encodings = ['latin-1', 'cp1252', 'iso-8859-1', 'utf-8']\n",
    "    df = None\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=\";\", encoding=encoding)\n",
    "            print(f\"  ‚úì Arquivo lido com encoding: {encoding}\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        print(f\"Erro: n√£o foi poss√≠vel ler {path} com nenhum encoding comum\")\n",
    "        return None\n",
    "    \n",
    "    # Verificar se tem a coluna text\n",
    "    if \"text\" not in df.columns:\n",
    "        print(f\"Erro: coluna 'text' n√£o encontrada em {path}\")\n",
    "        return None\n",
    "    \n",
    "    # Remover linhas com valores nulos\n",
    "    df = df.dropna(subset=[\"text\"])\n",
    "    \n",
    "    # Aplicar pr√©-processamento\n",
    "    df[\"text_preproc\"] = df[\"text\"].apply(lambda x: preprocess_operations(x, preprocess_params))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Carregar todos os dados de teste\n",
    "test_datasets = {}\n",
    "\n",
    "for file_name in TEST_FILES:\n",
    "    path = os.path.join(BASE_FOLDER_TEST, file_name)\n",
    "    print(f\"\\nCarregando: {file_name}\")\n",
    "    \n",
    "    df = load_and_preprocess_test(path)\n",
    "    \n",
    "    if df is not None:\n",
    "        test_datasets[file_name] = df\n",
    "        print(f\"  ‚úì {len(df):,} textos carregados e pr√©-processados\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Erro ao carregar {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9935ab",
   "metadata": {},
   "source": [
    "## Predi√ß√µes - ARCAICO vs MODERNO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2038f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDI√á√ïES - ARCAICO vs MODERNO\n",
      "================================================================================\n",
      "\n",
      "Melhor modelo: Logistic Regression\n",
      "Acur√°cia Hold-Out: 84.73%\n",
      "\n",
      "Total de textos para predi√ß√£o: 9,222\n",
      "\n",
      "üìä DISTRIBUI√á√ÉO DAS PREDI√á√ïES:\n",
      "   ‚Ä¢ moderno: 4,615 (50.04%)\n",
      "   ‚Ä¢ arcaico: 4,607 (49.96%)\n",
      "\n",
      "‚úÖ Predi√ß√µes salvas em: test_arcaico_moderno_predictions.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREDI√á√ïES - ARCAICO vs MODERNO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar o melhor modelo\n",
    "best_model_name = max(final_results.items(), key=lambda x: x[1]['holdout_acc'])[0]\n",
    "best_pipeline = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\nMelhor modelo: {best_model_name}\")\n",
    "print(f\"Acur√°cia Hold-Out: {final_results[best_model_name]['holdout_acc']*100:.2f}%\")\n",
    "\n",
    "# Carregar dados de teste\n",
    "df_test = test_datasets[\"test_arcaico_moderno.csv\"]\n",
    "X_test_texts = df_test[\"text_preproc\"].values\n",
    "\n",
    "print(f\"\\nTotal de textos para predi√ß√£o: {len(X_test_texts):,}\")\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "y_pred = best_pipeline.predict(X_test_texts)\n",
    "\n",
    "# Obter os labels originais (arcaico/moderno)\n",
    "df_train_original = pd.read_csv(os.path.join(BASE_FOLDER_TRAIN, \"train_arcaico_moderno.csv\"), sep=\";\", encoding='latin-1')\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_original[\"style\"])\n",
    "\n",
    "# Converter predi√ß√µes num√©ricas para labels\n",
    "predicted_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "# Adicionar predi√ß√µes ao dataframe\n",
    "df_test[\"predicted_style\"] = predicted_labels\n",
    "\n",
    "# Mostrar estat√≠sticas\n",
    "print(f\"\\nüìä DISTRIBUI√á√ÉO DAS PREDI√á√ïES:\")\n",
    "pred_counts = pd.Series(predicted_labels).value_counts()\n",
    "for label, count in pred_counts.items():\n",
    "    pct = (count / len(predicted_labels)) * 100\n",
    "    print(f\"   ‚Ä¢ {label}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Salvar resultado\n",
    "output_path = \"test_arcaico_moderno_predictions.csv\"\n",
    "df_output = df_test[[\"text\", \"predicted_style\"]]\n",
    "df_output.to_csv(output_path, sep=\";\", index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n‚úÖ Predi√ß√µes salvas em: {output_path}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d91e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDI√á√ïES - COMPLEXO vs SIMPLES\n",
      "================================================================================\n",
      "\n",
      "Melhor modelo: Logistic Regression\n",
      "Acur√°cia Hold-Out: 83.11%\n",
      "\n",
      "Total de textos para predi√ß√£o: 8,356\n",
      "\n",
      "üìä DISTRIBUI√á√ÉO DAS PREDI√á√ïES:\n",
      "   ‚Ä¢ simples: 4,275 (51.16%)\n",
      "   ‚Ä¢ complexo: 4,081 (48.84%)\n",
      "\n",
      "‚úÖ Predi√ß√µes salvas em: test_complexo_simples_predictions.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREDI√á√ïES - COMPLEXO vs SIMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar o melhor modelo\n",
    "best_model_name_cs = max(final_results_cs.items(), key=lambda x: x[1]['holdout_acc'])[0]\n",
    "best_pipeline_cs = best_models_cs[best_model_name_cs]\n",
    "\n",
    "print(f\"\\nMelhor modelo: {best_model_name_cs}\")\n",
    "print(f\"Acur√°cia Hold-Out: {final_results_cs[best_model_name_cs]['holdout_acc']*100:.2f}%\")\n",
    "\n",
    "# Carregar dados de teste\n",
    "df_test_cs = test_datasets[\"test_complexo_simples.csv\"]\n",
    "X_test_texts_cs = df_test_cs[\"text_preproc\"].values\n",
    "\n",
    "print(f\"\\nTotal de textos para predi√ß√£o: {len(X_test_texts_cs):,}\")\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "y_pred_cs = best_pipeline_cs.predict(X_test_texts_cs)\n",
    "\n",
    "# Obter os labels originais (complexo/simples)\n",
    "df_train_original_cs = pd.read_csv(os.path.join(BASE_FOLDER_TRAIN, \"train_complexo_simples.csv\"), sep=\";\")\n",
    "le_cs = LabelEncoder()\n",
    "le_cs.fit(df_train_original_cs[\"style\"])\n",
    "\n",
    "# Converter predi√ß√µes num√©ricas para labels\n",
    "predicted_labels_cs = le_cs.inverse_transform(y_pred_cs)\n",
    "\n",
    "# Adicionar predi√ß√µes ao dataframe\n",
    "df_test_cs[\"predicted_style\"] = predicted_labels_cs\n",
    "\n",
    "# Mostrar estat√≠sticas\n",
    "print(f\"\\nüìä DISTRIBUI√á√ÉO DAS PREDI√á√ïES:\")\n",
    "pred_counts_cs = pd.Series(predicted_labels_cs).value_counts()\n",
    "for label, count in pred_counts_cs.items():\n",
    "    pct = (count / len(predicted_labels_cs)) * 100\n",
    "    print(f\"   ‚Ä¢ {label}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Salvar resultado\n",
    "output_path_cs = \"test_complexo_simples_predictions.csv\"\n",
    "df_output_cs = df_test_cs[[\"text\", \"predicted_style\"]]\n",
    "df_output_cs.to_csv(output_path_cs, sep=\";\", index=False)\n",
    "print(f\"\\n‚úÖ Predi√ß√µes salvas em: {output_path_cs}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a78a07",
   "metadata": {},
   "source": [
    "## Predi√ß√µes - LITERAL vs DIN√ÇMICO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96e0c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDI√á√ïES - LITERAL vs DIN√ÇMICO\n",
      "================================================================================\n",
      "\n",
      "Melhor modelo: SVM (LinearSVC)\n",
      "Acur√°cia Hold-Out: 83.68%\n",
      "\n",
      "Total de textos para predi√ß√£o: 9,242\n",
      "\n",
      "üìä DISTRIBUI√á√ÉO DAS PREDI√á√ïES:\n",
      "   ‚Ä¢ dinamico: 4,745 (51.34%)\n",
      "   ‚Ä¢ literal: 4,497 (48.66%)\n",
      "\n",
      "‚úÖ Predi√ß√µes salvas em: test_literal_dinamico_predictions.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREDI√á√ïES - LITERAL vs DIN√ÇMICO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar o melhor modelo\n",
    "best_model_name_ld = max(final_results_ld.items(), key=lambda x: x[1]['holdout_acc'])[0]\n",
    "best_pipeline_ld = best_models_ld[best_model_name_ld]\n",
    "\n",
    "print(f\"\\nMelhor modelo: {best_model_name_ld}\")\n",
    "print(f\"Acur√°cia Hold-Out: {final_results_ld[best_model_name_ld]['holdout_acc']*100:.2f}%\")\n",
    "\n",
    "# Carregar dados de teste\n",
    "df_test_ld = test_datasets[\"test_literal_dinamico.csv\"]\n",
    "X_test_texts_ld = df_test_ld[\"text_preproc\"].values\n",
    "\n",
    "print(f\"\\nTotal de textos para predi√ß√£o: {len(X_test_texts_ld):,}\")\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "y_pred_ld = best_pipeline_ld.predict(X_test_texts_ld)\n",
    "\n",
    "# Obter os labels originais (literal/dinamico)\n",
    "df_train_original_ld = pd.read_csv(os.path.join(BASE_FOLDER_TRAIN, \"train_literal_dinamico.csv\"), sep=\";\")\n",
    "le_ld = LabelEncoder()\n",
    "le_ld.fit(df_train_original_ld[\"style\"])\n",
    "\n",
    "# Converter predi√ß√µes num√©ricas para labels\n",
    "predicted_labels_ld = le_ld.inverse_transform(y_pred_ld)\n",
    "\n",
    "# Adicionar predi√ß√µes ao dataframe\n",
    "df_test_ld[\"predicted_style\"] = predicted_labels_ld\n",
    "\n",
    "# Mostrar estat√≠sticas\n",
    "print(f\"\\nüìä DISTRIBUI√á√ÉO DAS PREDI√á√ïES:\")\n",
    "pred_counts_ld = pd.Series(predicted_labels_ld).value_counts()\n",
    "for label, count in pred_counts_ld.items():\n",
    "    pct = (count / len(predicted_labels_ld)) * 100\n",
    "    print(f\"   ‚Ä¢ {label}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Salvar resultado\n",
    "output_path_ld = \"test_literal_dinamico_predictions.csv\"\n",
    "df_output_ld = df_test_ld[[\"text\", \"predicted_style\"]]\n",
    "df_output_ld.to_csv(output_path_ld, sep=\";\", index=False)\n",
    "print(f\"\\n‚úÖ Predi√ß√µes salvas em: {output_path_ld}\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
